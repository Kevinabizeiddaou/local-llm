# üß† Local Chat with Ollama + Streamlit

A lightweight, privacy-first chatbot that runs **entirely on your machine** using open-source transformer models from **[Ollama](https://ollama.com/)**.  
It supports **personas (‚ÄúTwists‚Äù)**, **memory summarization**, and **prompt-engineering** for realistic, context-aware conversations.

---

## ‚öôÔ∏è 1. Setup Instructions

### **Requirements**

- Python 3.9 or newer
- Ollama installed and running locally
  ```bash
  curl -fsSL https://ollama.com/install.sh | sh     # macOS/Linux
  ```
  or download the Windows installer from [ollama.com](https://ollama.com/download).

### **Model Download**

You can use any Ollama model. Example:

```bash
ollama pull llama3.1:8b
```

### **Python Dependencies**

```bash
pip install streamlit requests
```

### **Run the App**

```bash
python -m streamlit run app.py
```

Once running, open the provided local URL (usually `http://localhost:8501`).

---

## üí¨ 2. Usage Guide

1. **Choose a Persona (‚ÄúTwist‚Äù)** from the dropdown (e.g., Funny Sidekick, Sports Commentator, Mystery Detective).
2. Optionally, **write your own custom system prompt** to fully define the bot‚Äôs personality and behavior.
3. Type a message and hit Enter.
   - Your message and the assistant‚Äôs reply appear instantly.
   - Older messages are summarized automatically to maintain long-term context.
4. Check the **Debug / Context Info** section to view the active system prompt and rolling summary.

---

## üß© 3. Model Details

- **Default Model:** `llama3.1:8b` (meta‚Äôs Llama 3.1, 8B parameters)
- **Context Window:** 8,192 tokens
- **API:** Served locally via `http://localhost:11434/api/chat`
- **Architecture:** Streamlit frontend ‚Üí local Ollama HTTP backend

You can switch to other models by changing the ‚ÄúModel‚Äù field in the sidebar or setting an environment variable:

```bash
export OLLAMA_MODEL="mistral"
```

---

## üé≠ 4. Persona ‚ÄúTwist‚Äù Implementation

Each **Twist** is implemented as a **system prompt** prepended to every conversation.  
This defines how the model should ‚Äúact,‚Äù shaping its tone and response structure.  
Example snippets:

| Persona                      | System Prompt Summary                                                 |
| ---------------------------- | --------------------------------------------------------------------- |
| **Funny Sidekick**           | ‚ÄúBe a witty helper. Always include one short joke or pun.‚Äù            |
| **Sports Commentator**       | ‚ÄúTalk like a play-by-play announcer. Use sports metaphors.‚Äù           |
| **Mystery Detective (Noir)** | ‚ÄúSpeak in moody, first-person monologue. Treat questions like clues.‚Äù |

You can add your own by writing any prompt in the ‚ÄúCustom system prompt‚Äù box ‚Äî it overrides presets.

---

## üß± 5. Prompt-Engineering Techniques

| Technique                 | Description                                                | Implementation                                                       |
| ------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------- |
| **System Prompts**        | Define tone, role, and constraints.                        | First message in `messages[]` (system role).                         |
| **Context Summarization** | Keeps conversation memory under the model‚Äôs context limit. | `summarize_history()` compresses older turns into a rolling summary. |
| **Token Budgeting**       | Ensures context + reply fit in the model‚Äôs window.         | Tracks estimated tokens and truncates if needed.                     |
| **Recency Priority**      | Keeps the latest N turns verbatim for short-term memory.   | `KEEP_LAST_TURNS` parameter.                                         |
| **Streaming Responses**   | Displays model output in real-time for responsiveness.     | `stream=True` mode with incremental rendering.                       |

---

## üß© 6. Example Prompts

**Funny Sidekick:**

> You: Explain neural networks
>
> Bot: Sure! They‚Äôre like brainy spaghetti ‚Äî lots of tangled connections passing signals until you get a tasty output üòÑ

**Sports Commentator:**

> You: Summarize AI ethics
>
> Bot: It‚Äôs a tight match between innovation and responsibility! On one side, progress sprints ahead; on the other, fairness plays solid defense.

**Mystery Detective:**

> You: Describe data privacy
>
> Bot: The case was simple. Too much data in the wrong hands. I followed the trail ‚Äî encryption, consent, accountability. The usual suspects.

---

## üß∞ 7. Configuration Options

You can modify in `app.py`:
| Variable | Purpose | Default |
|-----------|----------|---------|
| `MAX_CONTEXT_TOKENS` | Context budget before summarizing | 7000 |
| `KEEP_LAST_TURNS` | Number of recent turns kept verbatim | 8 |
| `OLLAMA_URL` | Backend endpoint | `http://localhost:11434/api/chat` |
| `MODEL` | Active model name | `llama3.1:8b` |

---

## üßæ 8. Future Enhancements

- Download/export chat history
- Add dark/light theme toggle
- Support voice input (SpeechRecognition)
- Multi-persona memory per session

---

## üßë‚Äçüíª Author

Created by **Kevin Abi Zeid Daou** ‚Äî AUB Software Engineering Student (AI Track).  
This project demonstrates **prompt engineering**, **context management**, and **local transformer deployment** using **Ollama + Streamlit**.
